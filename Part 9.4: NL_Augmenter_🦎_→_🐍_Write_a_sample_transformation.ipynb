{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 9.4: NL-Augmenter  ü¶é ‚Üí üêç Write a sample transformation",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulabpatel/NLP_Basics/blob/main/Part%209.4%3A%20NL_Augmenter_%F0%9F%A6%8E_%E2%86%92_%F0%9F%90%8D_Write_a_sample_transformation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video walkthrough the code : https://www.youtube.com/watch?v=1fJ78OoCQCs\n",
        "\n",
        "Arxiv Paper : https://arxiv.org/pdf/2112.02721.pdf"
      ],
      "metadata": {
        "id": "YM5GepLx4dYN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMrWwwGdP4wO"
      },
      "source": [
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "     https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "\n",
        "\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2QcAG8bThqq"
      },
      "source": [
        "# NL-Augmenter Colab example \n",
        "\n",
        "  * Play with an existing **transformation** \n",
        "    * Write your own **transformation** \n",
        "  * Play with an existing **filter**  \n",
        "    * Write your own **filter**         \n",
        "\n",
        "Total running time: ~10 min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI_4yHCIAvQx"
      },
      "source": [
        "## Install NL-Augmenter from GitHub\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkv4WSJsI7YV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbfcad3-42a2-4822-b0bd-07a67ad03ed5"
      },
      "source": [
        "!git clone https://www.github.com/GEM-benchmark/NL-Augmenter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'NL-Augmenter' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9KCH1qpHjDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e483688-a6cc-4825-f6c3-70feae2b2293"
      },
      "source": [
        "cd NL-Augmenter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NL-Augmenter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMiID0kSE_Qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "512e5415-301a-4e25-a1f5-6ea512ce850c"
      },
      "source": [
        "!pip install -r requirements.txt --quiet\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz (13.7 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.7 MB 464 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.8.2)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (21.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.10.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.49.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.6)\n",
            "Requirement already satisfied: pathy in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.6.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.13)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (5.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkdXQrzKR0zY"
      },
      "source": [
        "## Load modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJcZGbR7JVFt"
      },
      "source": [
        "from transformations.butter_fingers_perturbation.transformation import ButterFingersPerturbation\n",
        "from transformations.change_person_named_entities.transformation import ChangePersonNamedEntities\n",
        "from transformations.replace_numerical_values.transformation import ReplaceNumericalValues\n",
        "from interfaces.SentenceOperation import SentenceOperation\n",
        "from interfaces.QuestionAnswerOperation import QuestionAnswerOperation\n",
        "from evaluation.evaluation_engine import evaluate, execute_model\n",
        "from tasks.TaskTypes import TaskType"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT8V407QBFYz"
      },
      "source": [
        "## Play with some existing transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZfjp0toJdHh",
        "outputId": "5f5011cd-1f31-436e-d872-cb82e1111fcf"
      },
      "source": [
        "t1 = ButterFingersPerturbation(max_outputs=3)\n",
        "t1.generate(\"Jason wants to move back to India by the end of next year.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Jasln wants to move back to India by the end od next year.',\n",
              " 'Jason wants to move back to India by thx end of ntxt year.',\n",
              " 'Jason wants to move back no India by tht end od next yeac.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_o9ktK9JwKs",
        "outputId": "c5e5902d-b25b-4eb6-c805-868ed95f1ba7"
      },
      "source": [
        "t2 = ChangePersonNamedEntities(max_outputs=2)\n",
        "t2.generate(\"Jason wants to move back to India by the end of next year.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Jason wants to move back to India by the end of next year.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1khspY1AH_j",
        "outputId": "4bde586e-6857-425a-e8ee-f0c4b1142c5f"
      },
      "source": [
        "t3 = ReplaceNumericalValues(max_outputs=1)\n",
        "t3.generate(\"Jason's 3 sisters want to move back to India\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Jason's 8 sisters want to move back to India\"]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2CB0LRbBWST"
      },
      "source": [
        "## Define a simple transformation\n",
        "Let's define a very basic transformation which just uppercases the sentence. \n",
        "\n",
        "This transformation could be used for many [tasks](https://github.com/GEM-benchmark/NL-Augmenter/blob/add_filters_for_contrast_sets/tasks/TaskTypes.py) including text classification and generation. So, we need to populate the `tasks` variable to `[TaskType.TEXT_CLASSIFICATION, TaskType.TEXT_TO_TEXT_GENERATION]`. That's it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BymdwQ3PJzg7"
      },
      "source": [
        "class MySimpleTransformation(SentenceOperation):\n",
        "  tasks = [TaskType.TEXT_CLASSIFICATION, TaskType.TEXT_TO_TEXT_GENERATION]\n",
        "  languages = [\"en\"]\n",
        "  \n",
        "  def generate(self, sentence):\n",
        "    return [sentence.upper()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkqtwqYlUWXV"
      },
      "source": [
        "my_transformation = MySimpleTransformation() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbbSJxJ8UbVz",
        "outputId": "15b0369e-d2c1-438b-946c-1e79f22ee74e"
      },
      "source": [
        "my_transformation.generate(\"John was n't the person I had n't imagined.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"JOHN WAS N'T THE PERSON I HAD N'T IMAGINED.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8682Ql9GOP0"
      },
      "source": [
        "\n",
        "Obviously this can barely be called a transformation. What could this really achieve? Duh. \n",
        "So, let's quickly compare the performance of a trained text classifier on a common test set, and a test set with MySimpleTransformation applied (or also called as a pertubed set) with this one line of code. And you need to hold your breadth for around 5 minutes!  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJlW0WnrVU0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3707540-61d4-4a37-80cf-b6c46c191f55"
      },
      "source": [
        "execute_model(MySimpleTransformation, \"TEXT_CLASSIFICATION\", percentage_of_examples=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading <imdb> dataset to evaluate <aychang/roberta-base-imdb> model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the performance of the model aychang/roberta-base-imdb on the test[:1%] split of the imdb dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:00<00:00, 70701.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy on this subset which has 250 examples = 96.0\n",
            "Applying transformation:\n",
            "Finished transformation! 250 examples generated from 250 original examples, with 250 successfully transformed and 0 unchanged (1.0 perturb rate)\n",
            "Here is the performance of the model on the transformed set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy on this subset which has 250 examples = 89.0\n",
            "Performance ={'model_name': 'aychang/roberta-base-imdb', 'split': 'test[:1%]', 'dataset_name': 'imdb', 'accuracy': 96.0, 'no_of_examples': 250, 'pt_accuracy': 89.0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 96.0,\n",
              " 'dataset_name': 'imdb',\n",
              " 'model_name': 'aychang/roberta-base-imdb',\n",
              " 'no_of_examples': 250,\n",
              " 'pt_accuracy': 89.0,\n",
              " 'split': 'test[:1%]'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_NeVxa0RKWx"
      },
      "source": [
        "### üï∫ Voila! The accuracy on the perturbed set has fallen by 6% with this simple transformation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4qmvF6sQRWu"
      },
      "source": [
        "So what happened internally? --> `execute_model` depending on the transformation type [SentenceOperation](https://github.com/GEM-benchmark/NL-Augmenter/blob/main/interfaces/SentenceOperation.py)) and the task you provided (TEXT_CLASSIFICATION) evaluated a pre-trained model of HuggingFace. In this case, a sentiment analysis model [aychang/roberta-base-imdb](https://huggingface.co/aychang/roberta-base-imdb) was chosen and evaluated on 1% of the [IMDB dataset](https://huggingface.co/datasets/imdb) with and without the transformation to check if the sentiment is predicted correctly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmOP8B3-TW0i"
      },
      "source": [
        "If you want to evaluate this on your own model and dataset, you can pass the parameters as shown below in the `execute_model` method. Note that we obviously can't support each and every model type and dataset type and hence some models and datasets might require refactoring in the `evaluation_engine` class from your side and we are happy to help. üòä"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKcUmvYzYKAJ"
      },
      "source": [
        "# Here are the different parameters which are used as defaults!\n",
        "# execute_model(MySimpleTransformation, \"TEXT_CLASSIFICATION\", \"en\", model_name = \"aychang/roberta-base-imdb\", dataset=\"imdb\", percentage_of_examples=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHuYXB6OWNiU"
      },
      "source": [
        "##  A Model Based Transformation\n",
        "We don't want to restrict ourselves with just string level changes! We want to do more, don't we? So, let's use a pre-trained paraphrase generator to transform question answering examples. There is an exisiting interface [QuestionAnswerOperation](https://github.com/GEM-benchmark/NL-Augmenter/blob/main/interfaces/QuestionAnswerOperation.py) which takes as input the context, the question and the answer as inputs. Let's use that to augment our training data for question answering! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3DehjWXYwnn"
      },
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
        "\n",
        "class MySecondTransformation(QuestionAnswerOperation):\n",
        "  tasks = [TaskType.QUESTION_ANSWERING, TaskType.QUESTION_GENERATION]\n",
        "  languages = [\"en\"]\n",
        "\n",
        "  def __init__(self, max_outputs=5):\n",
        "    super().__init__()\n",
        "    model_name=\"prithivida/parrot_paraphraser_on_T5\"\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)  \n",
        "    self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    self.max_outputs = max_outputs\n",
        "\n",
        "  def generate(self, context, question, answers): # Note that the choice of inputs for 'generate' is consistent with those in QuestionAnswerOperation\n",
        "    \n",
        "    # Let's call the HF model to generate a paraphrase for the question\n",
        "    paraphrase_input = question\n",
        "    batch = self.tokenizer([paraphrase_input],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\")\n",
        "    translated = self.model.generate(**batch,max_length=60,num_beams=10, num_return_sequences=self.max_outputs, temperature=1.5)\n",
        "    paraphrased_questions = self.tokenizer.batch_decode(translated, skip_special_tokens=True) \n",
        "\n",
        "    # context = \"Apply your own logic here\"\n",
        "    # answers = \"And here too :)\"\n",
        "\n",
        "    # return the list of new question-answering examples\n",
        "    return [(context, paraphrase, answers) for paraphrase in paraphrased_questions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84G9YzdGblfP"
      },
      "source": [
        "t4 = MySecondTransformation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFY0lGA2lIqy",
        "outputId": "f844103c-d9df-4883-d641-877d0515103f"
      },
      "source": [
        "t4.generate(context=\"Mumbai, Bengaluru, New Delhi are among the many famous places in India.\", \n",
        "            question=\"What are the famous places we should not miss in India?\", \n",
        "            answers=[\"Mumbai\", \"Bengaluru\", \"Delhi\", \"New Delhi\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1818: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  next_indices = next_tokens // vocab_size\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Mumbai, Bengaluru, New Delhi are among the many famous places in India.',\n",
              "  'recommend some of the best places to visit in India?',\n",
              "  ['Mumbai', 'Bengaluru', 'Delhi', 'New Delhi']),\n",
              " ('Mumbai, Bengaluru, New Delhi are among the many famous places in India.',\n",
              "  'can you list the best places to visit in India?',\n",
              "  ['Mumbai', 'Bengaluru', 'Delhi', 'New Delhi']),\n",
              " ('Mumbai, Bengaluru, New Delhi are among the many famous places in India.',\n",
              "  'can you list the top 10 places to visit in India?',\n",
              "  ['Mumbai', 'Bengaluru', 'Delhi', 'New Delhi']),\n",
              " ('Mumbai, Bengaluru, New Delhi are among the many famous places in India.',\n",
              "  'list some of the best places to visit in India?',\n",
              "  ['Mumbai', 'Bengaluru', 'Delhi', 'New Delhi']),\n",
              " ('Mumbai, Bengaluru, New Delhi are among the many famous places in India.',\n",
              "  'can you list the places we should not miss in India?',\n",
              "  ['Mumbai', 'Bengaluru', 'Delhi', 'New Delhi'])]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOCoNfaV6F9l"
      },
      "source": [
        "Voila! Seems like you have created a new training example now for question-answering and question-generation! üéâ üéä üéâ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WF-JtPd6wAm"
      },
      "source": [
        "#Now you are all ready to contribute a transformation to [NL-Augmenter ü¶é ‚Üí üêç](https://github.com/GEM-benchmark/NL-Augmenter)! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4e9Y57h0wh-"
      },
      "source": [
        "## What is this deal with filters?\n",
        "So, just the way transformations can transform examples of text, filters can identify whether an example follows some pattern of text! The only difference is that while transformations return another example of the same input format, filters return True or False!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDpDvOjv2Yx9"
      },
      "source": [
        "sentence --> SentenceOperation.**generate**(sentence) --> List of perturbed sentence\n",
        "\n",
        "sentence --> SentenceOperation.**filter**(sentence)  --> TRUE/FALSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChHoHeq8CGXX"
      },
      "source": [
        "#So, let's play with some existing filters! \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfUvpkSN0BKB"
      },
      "source": [
        "from filters.keywords import TextContainsKeywordsFilter\n",
        "from filters.length import TextLengthFilter, SentenceAndTargetLengthFilter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le0p5dsBDGA1"
      },
      "source": [
        "The `TextLengthFilter` accepts an input sentence if the length of the input sentence is within the initialised range. Let's initialise this filter to accept all sentences with length greater than 10 tokens!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb2u3gsE0d_n"
      },
      "source": [
        "f1 = TextLengthFilter(\">\", 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK0xTRBsFCdQ",
        "outputId": "f9b2c78f-d73f-4bff-c958-74e4c2bc7d5e"
      },
      "source": [
        "f1.filter(\"This sentence is long enough to pass while you think of implementing your own filter!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'This'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'sentence'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'is'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'long'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'enough'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'to'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'pass'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'while'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'you'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'think'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'of'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'implementing'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'your'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'own'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'filter'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token '!'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diGI4EaOFSun",
        "outputId": "bf3de85e-9c90-4dd1-d932-d101b53d0db0"
      },
      "source": [
        "f1.filter(\"This one's too short!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'This'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'one'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token ''s'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'too'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'short'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token '!'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZMYN4COFiY8"
      },
      "source": [
        "Let's say you have a lot of paraphrasing data and you intend to train a paraphrase generator to convert longer sentences to shorter ones! Check how the `SentenceAndTargetLengthFilter` can be used for this!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H27VEe8pFYMl"
      },
      "source": [
        "f2 = SentenceAndTargetLengthFilter([\">\", \"<\"], [10,8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql1ZSsyjG_Y7",
        "outputId": "729323be-5650-43a6-b594-1563e13fe495"
      },
      "source": [
        "f2.filter(\"That show is going to take place in front of immensely massive crowds.\", \n",
        "          \"Large crowds would attend the show.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'That'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'show'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'is'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'going'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'to'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'take'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'place'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'in'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'front'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'of'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'immensely'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'massive'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'crowds'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'Large'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'crowds'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'would'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'attend'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'show'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKkFIgAtHsB-",
        "outputId": "3109d555-a5bb-4b40-aa28-b436bca33876"
      },
      "source": [
        "f2.filter(\"The film was nominated for the Academy Award for Best Art Direction.\", \n",
        "          \"The movie was a nominee for the Academy Award for Best Art Direction.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'The'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'film'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'was'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'nominated'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'for'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'Academy'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'Award'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'for'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'Best'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'Art'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'Direction'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'The'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'movie'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'was'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'a'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'nominee'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'for'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'Academy'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'Award'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'for'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'Best'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'Art'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'Direction'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAScGDfaJKa9"
      },
      "source": [
        "Okay, now that you've said to yourself that these filters are too basic, let's try to make a simple and interesting one! \n",
        "\n",
        "Let's define a filter which selects question-answer pairs which share a low lexical overlap between the question and the context!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7b1mg2ZJcsc"
      },
      "source": [
        "import spacy\n",
        "class LowLexicalOverlapFilter(QuestionAnswerOperation):\n",
        "  tasks = [TaskType.QUESTION_ANSWERING, TaskType.QUESTION_GENERATION]\n",
        "  languages = [\"en\"]\n",
        "  \n",
        "  def __init__(self, threshold=3):\n",
        "    super().__init__()\n",
        "    self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "    self.threshold = threshold\n",
        "\n",
        "  def filter(self, context, question, answers): \n",
        "    # Note that the only difference between a filter and a transformation is this method! \n",
        "    # The inputs remain the same!\n",
        "    \n",
        "    question_tokenized = self.nlp(question, disable=[\"parser\", \"tagger\", \"ner\"])\n",
        "    context_tokenized = self.nlp(context, disable=[\"parser\", \"tagger\", \"ner\"])\n",
        "    \n",
        "    q_tokens = set([t.text for t in question_tokenized])\n",
        "    c_tokens = set([t.text for t in context_tokenized])\n",
        "    \n",
        "    low_lexical_overlap = len(q_tokens.intersection(c_tokens)) > self.threshold\n",
        "    return low_lexical_overlap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtKYvAr2MbSf"
      },
      "source": [
        "f3 = LowLexicalOverlapFilter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1np6KirQGZc",
        "outputId": "2ca4865f-6460-4c7f-cea2-1b2c76ba647b"
      },
      "source": [
        "f3.filter(\"New York, is the most populous city in the United States.\",\n",
        "          \"Which is the most populous city of the United States?\",\n",
        "          [\"New York\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'Which'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'is'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'most'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'populous'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'city'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'of'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'United'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'States'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token '?'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'New'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'York'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'is'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'most'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'populous'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'city'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'in'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'United'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'States'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbFOyYHUQVnk",
        "outputId": "1783cd2b-afd7-4625-92d9-b5ba10c87004"
      },
      "source": [
        "f3.filter(\"New York, is the most populous city in the United States.\",\n",
        "          \"Which city has the largest population in the US?\",\n",
        "          [\"New York\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'Which'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'city'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'has'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'largest'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'population'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'in'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'US'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token '?'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'New'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'York'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'is'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'most'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'populous'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'city'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'in'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'United'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token 'States'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "[W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE0NY7NKRGfE"
      },
      "source": [
        "That's it!  So you have created a new filter which can separate the hard examples from the easy one! üéâ üéä üéâ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKHd2QC_RkwB"
      },
      "source": [
        "#Now go ahead and contribute a nice filter to [NL-Augmenter ü¶é ‚Üí üêç](https://github.com/GEM-benchmark/NL-Augmenter)! "
      ]
    }
  ]
}